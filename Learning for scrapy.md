# Reference links for existed framework learning

## 断点续传
找一个　内存/txt 记录断点前的状态, 再次爬取时可利用与记录比较的方法筛选出哪些是没有爬过的，然后进行爬取。主要是筛选url。<br>
另外可以学习redis模块建队列，也是增量爬虫常用的一种。And bloomfilter<br>
## 增量爬虫
简单版：利用数据库无法加入相同记录的特点，将爬取的内容直接存取，ｅｒｒｏｒ则跳过（我看到的例子是还含有分时段定时爬取的，应该用于防ban?）,再深一点应该是直接放到内存做增量爬取的，略深。增量法爬虫的优点是爬取出来的内容将具有时效性，因为即使有些url你的内容已经爬过了，可能过几天内容有更新过了，增量法可以消除这个bug。
## 按优先级队列，看似是利用scrapy本身的多线程优先级进行排队，最新状态的爬虫放在最优先的位置。可能用了算法来建立优先级。
## scrapy的改编框架webmagic
## 用哈希函数优化爬虫的，有兴趣的同学可以再了解一下。
## scrapy本身含有断电，断网，暂停后续传的机制（只需一些配置和scrapy crawler projectname + ....特殊的命令行即可做到)
## 定时爬取
![]("https://blog.csdn.net/qq_21768483/article/details/78725481" "定时爬取")







